[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Statistics",
    "section": "",
    "text": "Preface\nThis book is not for statisticians. It is for the large majority of other users of statistics. You need some overview of what methods there are to begin with. You probably don’t have time to learn all the basics first and build your knowledge from the ground up. That is why I want to take a different, hopefully more pragmatic approach in this book."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Applied Statistics",
    "section": "",
    "text": "For example, openstax free Introductory Statistics.↩︎"
  },
  {
    "objectID": "probabilitydistributions.html",
    "href": "probabilitydistributions.html",
    "title": " Probability Distributions",
    "section": "",
    "text": "Functions describing the probability of possible outcomes.\n\nJust because something is random does not mean every outcome is equally likely. Probability distributions describe how likely each possible outcome of a random event is.\nTo get started, familiarize yourself with at least these three probability distributions:\n\nThe  normal distribution (continuous processes with a central tendency)\nThe  Poisson distribution (counts)\nThe  binomial distribution (ratios, proportions, binary data)\n\nThe video below introduces these and provides some intuition.\n\n\n\n\nOpen video in a new tab \nThere are many other useful probability distributions which we will learn about as we progress. Here is a quick overview of what is yet to come:\n\n…\n…\n(to be included)"
  },
  {
    "objectID": "normaldistribution.html#properties",
    "href": "normaldistribution.html#properties",
    "title": " Normal Distribution",
    "section": "Properties",
    "text": "Properties\n\nProbability density functionCumulative distribution functionQuantile function\n\n\nWhat is the height of the curve at a given point?\n\n\n\n\n\nFigure 1: Probability density of a standard normal distribution (\\(\\mu = 0\\), \\(\\sigma^2 = 1\\)).\n\n\n\n\nMathematical formula:\n\\[\\begin{align}\n  \\tag{1}\\label{dnorm}\n  \\mathcal{N}(\\mu, \\, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} e ^ {- \\frac{(x - \\mu)^2}{2\\sigma^2}}\n\\end{align}\\]\nCode:\n\nRPython\n\n\n\ndnorm(x, mean, sd)\n\n\n\n\nfrom scipy.stats import norm\nnorm.pdf(x, loc, scale)\n\nNote: loc refers to the mean, scale to the standard deviation.\n\n\n\n\n\n\n\n\n\nSymbols\n\n\n\n\n\n\n\\(\\mathcal{N}\\): A normal distribution\n\\(\\mu\\): The mean of the distribution\n\\(\\sigma^2\\): The variance of the distribution\n\\(\\pi\\): The constant 3.141593… (why?)\n\\(e\\): The constant 2.718282… (why?)\n\\(x\\): The value of a given observation\n\n\n\n\n\n\n\n\n\n\nExplanation\n\n\n\n\n\nThe normal distribution is a continuous probability distribution. This has an important implication: The \\(y\\)-axis in Figure 1 shows probability density, not probability.1 If you actually want to calculate probabilities, you have to calculate the surface area under the curve, which can be done with the cumulative distribution function\nThe standard normal distribution refers to a normal distribution with \\(\\mu = 0\\) and \\(\\sigma^2 = 1\\).\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n\n\nRPython\n\n\nThe top of the curve (\\(x = 0\\)) in Figure 1 has a height of:\n\ndnorm(x = 0, mean = 0, sd = 1)\n\n[1] 0.3989423\n\n\nAt \\(x = 2\\), the curve has a height of:\n\ndnorm(2, 0, 1)\n\n[1] 0.05399097\n\n\n\n\nThe top of the curve (\\(x = 0\\)) in Figure 1 has a height of:\n\nfrom scipy.stats import norm\nnorm.pdf(x = 0, loc = 0, scale = 1)\n\n0.3989422804014327\n\n\nAt \\(x = 2\\), the curve has a height of:\n\nnorm.pdf(2, 0, 1)\n\n0.05399096651318806\n\n\n\n\n\n\n\n\n\n\nWhat is the probability (surface area) up till a given point \\(z\\)?\n\n\n\n\n\nFigure 2: Cumulative distribution function of a standard normal (\\(\\mu = 0\\), \\(\\sigma^2 = 1\\)).\n\n\n\n\nMathematical formula:\n\\[\\begin{align}\n  \\tag{2}\\label{pnorm}\n  \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\int_{-\\infty}^z e^{- \\frac{(x - \\mu)^2}{2 \\sigma^2}} \\mathrm{d}x\n\\end{align}\\]\nCode:\n\nRPython\n\n\n\npnorm(z, mean, sd)\n\n\n\n\nfrom scipy.stats import norm\nnorm.cdf(z, loc, scale)\n\nNote: loc refers to the mean, scale to the standard deviation.\n\n\n\n\n\n\n\n\n\nSymbols\n\n\n\n\n\n\n\\(\\pi\\): The constant 3.141593… (why?)\n\\(\\sigma^2\\): The variance of the distribution\n\\(\\int_{-\\infty}^z \\dots \\mathrm{d}z\\): The integral (surface area) from \\(-\\infty\\) to \\(z\\)\n\\(e\\): The constant 2.718282… (why?)\n\\(\\mu\\): The mean of the distribution\n\\(x\\): The value of a given observation\n\n\n\n\n\n\n\n\n\n\nExplanation\n\n\n\n\n\nThe cumulative distribution function (CDF) is the integral of the probability density function up till a given point \\(z\\). It can be used to calculate the probability of an observation being less than a certain value. To obtain the opposite—the chance of an observation being equal to or greater than a certain value, simply use \\(1\\) minus the CDF. Finally, to compute the chance of being within a certain range, you can subtract the CDF of the larger value, from the CDF of the smaller value (see the examples below).\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n\n\nRPython\n\n\nIf adult males in The Netherlands are \\(1.838\\) m on average, with a standard deviation of \\(7\\) cm, the chance of being 2 meters or taller is:\n\n1 - pnorm(2, 1.838, 0.07)\n\n[1] 0.01032603\n\n\nTo compute the probability of a range, just subtract the smaller value from the greater:\n\n# Probability of being within 5 cm from the mean\npnorm(1.888, 1.838, 0.07) - pnorm(1.788, 1.838, 0.07)\n\n[1] 0.5249495\n\n\n\n\nVisual interpretation\n\n\n\n\n\n\n\nThe probability of a value of \\(z \\leq 1.5\\) in Figure 2 is given by:\n\npnorm(-1.5, mean = 0, sd = 1)\n\n[1] 0.0668072\n\n\n\n\nIf adult males in The Netherlands are \\(1.838\\) m on average, with a standard deviation of \\(7\\) cm, the chance of being 2 meters or taller is:\n\nfrom scipy.stats import norm\n1 - norm.cdf(2, 1.838, 0.07)\n\n0.010326027446208808\n\n\nTo compute the probability of a range, just subtract the smaller value from the greater:\n\n# Probability of being within 5 cm from the mean\nnorm.cdf(1.888, 1.838, 0.07) - norm.cdf(1.788, 1.838, 0.07)\n\n0.5249494759460464\n\n\n\n\nVisual interpretation\n\n\n\n\n\n\n\nThe probability of a value of \\(z \\leq 1.5\\) in Figure 2 is given by:\n\nnorm.cdf(-1.5, loc = 0, scale = 1)\n\n0.06680720126885807\n\n\n\n\n\n\n\n\n\n\nUp till what point does the surface area equal a given probability \\(p\\)?\n\n\n\n\n\nFigure 3: Quantile function of a standard normal distribution (\\(\\mu = 0\\), \\(\\sigma^2 = 1\\)).\n\n\n\n\nMathematical formula:\n\\[\\begin{align}\n  \\tag{3}\\label{qnorm}\n  \\mu +  \\sqrt{2\\sigma^2} \\cdot \\mathrm{erf}^{-1}(2p - 1)\n\\end{align}\\]\nCode:\n\nRPython\n\n\n\nqnorm(probability, mean, sd)\n\n\n\n\nfrom scipy.stats import norm\nnorm.ppf(propbability, loc, scale)\n\nNote: loc refers to the mean, scale to the standard deviation.\n\n\n\n\n\n\n\n\n\nSymbols\n\n\n\n\n\n\n\\(\\mu\\): The mean of the distribution\n\\(\\sigma^2\\): The variance of the distribution\n\\(\\mathrm{erf}^{-1}\\): The inverse error function, which has no closed form, but can be approximated numerically\n\n\n\n\n\n\n\n\n\n\nExplanation\n\n\n\n\n\nThe quantile function is the inverse of the cumulative distribution function. It tells you up till what point the probability equals \\(p\\). To obtain the opposite—the point after which the probability equals a certain value, simply compute the quantile of \\(1\\) minus the probability (see the examples below).\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n\n\nRPython\n\n\nHow tall (at least) are the largest 5% of adult males in The Netherlands?(Assuming a normal distribution with \\(\\mu = 1.838\\), \\(\\sigma = 0.07\\))\n\nqnorm(0.95, 1.838, 0.07)\n\n[1] 1.95314\n\n\nWithin which heights are 95% of adult males in The Netherlands?\n\nqnorm(c(0.025, 0.975), 1.838, 0.07)\n\n[1] 1.700803 1.975197\n\n\n\n\nHow tall (at least) are the largest 5% of adult males in The Netherlands?(Assuming a normal distribution with \\(\\mu = 1.838\\), \\(\\sigma = 0.07\\))\n\nfrom scipy.stats import norm\nnorm.ppf(0.95, 1.838, 0.07)\n\n1.953139753886603\n\n\nWithin which heights are 95% of adult males in The Netherlands?\n\nresult = norm.ppf([0.025, 0.975], 1.838, 0.07)\nprint(result)\n\n[1.70080252 1.97519748]"
  },
  {
    "objectID": "normaldistribution.html#quiz",
    "href": "normaldistribution.html#quiz",
    "title": " Normal Distribution",
    "section": "Quiz",
    "text": "Quiz\n\nQuestion 1Answer\n\n\nUsing the following code, you can plot a normal distribution with \\(\\mu = 1\\) and \\(\\sigma^2 = 0.09\\):\n\nRPython\n\n\n\nx &lt;- seq(0, 2, 0.01)\ny &lt;- dnorm(x, 1, 0.3)\nplot(x, y, type = \"l\")\n\n\n\n\n\n\n\n\n\n\n\n\nTry it in browser\n\n\n\n\n\nLoading\n  webR...\n\n\n  \n\n\n(This is an experimental feature. I recommend just running RStudio in the background while reading this book, but perhaps this can be useful if you are reading this on your phone for example… Feedback is welcome!)\n\n\n\n\n\n\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\nx = np.arange(0, 2, 0.01)\ny = norm.pdf(x, loc = 1, scale = 0.3)\nplt.plot(x, y)\nplt.show()\n\n\n\n\n\n\n\nIf you look at the \\(y\\)-axis of the plot, you’ll see the largest value is greater than \\(1\\), even though probabilities cannot exceed \\(1\\). Why is this the case?\n\n\n\n\n\n\nHint\n\n\n\n\n\nUnfold the explanation part of the probability density function.\n\n\n\n\n\nThe axis shows probability density, not probability. To obtain a probability, you first have to compute the surface area under the curve (cumulative distribution function). The total area under the curve of a probability distribution always equals \\(1\\).\nImagine a rectangular probability distribution (also called a uniform distribution):\n\n\n\n\n\nFigure 4: Uniform distribution on \\(a = 0.5\\), \\(b = 1\\).\n\n\n\n\nThe probability density goes all the way to \\(2\\), but since there is only a non-zero probability density between \\(0.5\\) and \\(1\\), the surface area under the curve still sums up to a total probability of \\(2 \\times 0.5 = 1\\).\n\n\n\n\nQuestion 2Answer\n\n\nIf adult female height in The Netherlands can be reasonably approximated by a normal distribution with \\(\\mu = 1.704\\) m and \\(\\sigma = 7\\) cm, what percentage is taller than \\(1.80\\) m?\n\n\n\n\n\n\nHint\n\n\n\n\n\nFirst, convert both to the same unit: \\(\\mu = 1.704\\) m, \\(\\sigma = 0.07\\) m.(Converting both to cm works as well.)\nThen, use the code from the previous exercise to plot the distribution. This is the best way to get a feel for it.\n\n\n\n\n\nNow you can already see from the figure that the answer must be some small number, as \\(1.8\\) is already well into the tails.\nThe question can be answered with the cumulative distribution function.\n\n\n\n\n\n\n\n\n\nTry it in browser\n\n\n\n\n\nLoading\n  webR...\n\n\n  \n\n\n(This is an experimental feature. I recommend just running RStudio in the background while reading this book, but perhaps this can be useful if you are reading this on your phone for example… Feedback is welcome!)\n\n\n\n\n\nUse \\(1\\) minus the cumulative distribution function to find the probability in the right tail of the distribution.\n\nRPython\n\n\n\n1 - pnorm(1.8, 1.704, 0.07)\n\n[1] 0.0851207\n\n\n\n\n\nfrom scipy.stats import norm\n1 - norm.cdf(1.8, 1.704, 0.07)\n\n0.0851206979548067\n\n\n\n\n\nAbout \\(8.5\\%\\) is taller than \\(1.8\\) m.\n\n\n\n\n\n\n\n\n\nQuestion 3Answer\n\n\nSuppose the beak length of medium ground finches (Geospiza fortis) can be reasonably approximated using a normal distribution with a mean of 11.0 mm with a standard deviation of 0.5 mm. Within what range do you expect 90% to fall?\n\n\n\n\n\nExample of a medium ground finch.\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the code from exercise 1 to plot the probability distribution. This is the best way to get a feel for it.\n\n\n\n\n\nNow you can already see what would be a plausible answer… perhaps around 10–12 mm?\nThe question can be answered with the quantile function.\n\n\n\n\n\n\n\n\n\nTry it in browser\n\n\n\n\n\nLoading\n  webR...\n\n\n  \n\n\n(This is an experimental feature. I recommend just running RStudio in the background while reading this book, but perhaps this can be useful if you are reading this on your phone for example… Feedback is welcome!)\n\n\n\n\n\nUse the quantile function from \\(5\\%\\) to \\(95\\%\\) to find the middle \\(90\\%\\):\n\nRPython\n\n\n\nqnorm(c(0.05, 0.95), 11, 0.5)\n\n[1] 10.17757 11.82243\n\n\n\n\n\nfrom scipy.stats import norm\nresult = norm.ppf([0.05, 0.95], 11, 0.5)\nprint(result)\n\n[10.17757319 11.82242681]\n\n\n\n\n\n\\(95\\%\\) of beak lengths are expected to be within \\(10.2\\) and \\(11.8\\) mm."
  },
  {
    "objectID": "normaldistribution.html#footnotes",
    "href": "normaldistribution.html#footnotes",
    "title": " Normal Distribution",
    "section": "",
    "text": "For an explanation, please refer to the video.↩︎"
  },
  {
    "objectID": "poissondistribution.html#properties",
    "href": "poissondistribution.html#properties",
    "title": " Poisson Distribution",
    "section": "Properties",
    "text": "Properties\n\nProbability mass functionCumulative distribution functionQuantile function\n\n\nWhat is the probability of a given count \\(k\\)?\n\n\n\n\n\nFigure 1: Probability mass function of a Poisson distribution with \\(\\lambda = 3\\).\n\n\n\n\nMathematical formula:\n\\[\\begin{align}\n  \\tag{1}\\label{dpois}\n  \\frac{\\lambda^k e^{-\\lambda}}{k!}\n\\end{align}\\]\nCode:\n\nRPython\n\n\n\ndpois(k, lambda)\n\n\n\n\nfrom scipy.stats import poisson\npoisson.pmf(k, mu)\n\nNote: mu refers to the mean (\\(\\lambda\\)).\n\n\n\n\n\n\n\n\n\nSymbols\n\n\n\n\n\n\n\\(\\lambda\\): The rate parameter, representing both mean and variance\n\\(e\\): The constant 2.718282… (why?)\n\\(k\\): The value of a given count\n\\(!\\): Factorial (e.g., \\(3! = 3 \\times 2 \\times 1\\))\n\n\n\n\n\n\n\n\n\n\nExplanation\n\n\n\n\n\nThe Poisson distribution is a discrete probability distribution. There are only certain possible values for the outcome, like \\(0, 1, 2, \\dots\\), but not \\(1.5\\) or \\(2.01\\). Hence, you can directly read probabilities off the \\(y\\)-axis in Figure 1.\nThe correct term for a probability function of a discrete distribution is a probability mass function, though it is common in literature to see people call both discrete and continuous versions a probability density function.\nNote that while it looks like the probability goes to zero after \\(k = 10\\) in Figure 1, it never exactly reaches zero. The probability is just too small to be visible in the figure.\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nRPython\n\n\nThe probability of \\(k = 4\\) in Figure 1 is:\n\ndpois(4, lambda = 3)\n\n[1] 0.1680314\n\n\n\n\nThe probability of \\(k = 4\\) in Figure 1 is:\n\nfrom scipy.stats import poisson\npoisson.pmf(4, mu = 3)\n\n0.16803135574154085\n\n\n\n\n\n\n\n\n\n\nWhat is the probability up till a given point \\(k\\)?\n\n\n\n\n\nFigure 2: Cumulative distribution function of a Poisson distribution with \\(\\lambda = 3\\).\n\n\n\n\nMathematical formula:\n\\[\\begin{align}\n  \\tag{2}\\label{ppois}\n  \\sum_{i = 0}^k \\frac{\\lambda^i e^{-\\lambda}}{i!}\n\\end{align}\\]\nCode:\n\nRPython\n\n\n\nppois(k, mean, sd)\n\n\n\n\nfrom scipy.stats import poisson\npoisson.cdf(k, mu)\n\nNote: mu refers to the rate \\(\\lambda\\).\n\n\n\n\n\n\n\n\n\nSymbols\n\n\n\n\n\n\n\\(\\lambda\\): The rate parameter, representing both mean and variance\n\\(e\\): The constant 2.718282… (why?)\n\\(k\\): The value of a given count\n\\(!\\): Factorial (e.g., \\(3! = 3 \\times 2 \\times 1\\))\n\\(\\sum\\): Summation (compute for \\(0, 1, \\dots, k\\) and add all results)\n\n\n\n\n\n\n\n\n\n\nExplanation\n\n\n\n\n\nThe cumulative distribution function (CDF) is the sum of the probability mass function up till a given point \\(k\\). It can be used to calculate the probability of a count being no greater than a certain value. To obtain the opposite—the chance of an observation greater than a certain value, simply use \\(1\\) minus the CDF. Finally, to compute the chance of being within a certain range, you can subtract the CDF of the larger value, from the CDF of the smaller value (see the examples below).\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n\n\nRPython\n\n\nIf there are on average 5 falling stars in the sky every night, the chance of observing no more than 3 on a given night is equal to:\n\nppois(3, lambda = 5)\n\n[1] 0.2650259\n\n\nAnd the chance of observing at least 3 is equal to:\n\n1 - ppois(2, lambda = 5)\n\n[1] 0.875348\n\n\n(This works because you start with 1 (100%) exclude all options resulting in less than 3.)\nTo compute the probability of a certain range, just subtract the smaller value from the greater:\n\n# Probability of 2 to 4 falling stars:\nppois(4, lambda = 5) - ppois(1, lambda = 5)\n\n[1] 0.4000656\n\n\n(This works because you start with all possibilities up till 4 and then exclude all options resulting in less than 2.)\nThe probability of a value of \\(k \\leq 3\\) in Figure 2 is given by:\n\nppois(3, lambda = 3)\n\n[1] 0.6472319\n\n\n\n\nIf there are on average 5 falling stars in the sky every night, the chance of observing no more than 3 on a given night is equal to:\n\nfrom scipy.stats import poisson\npoisson.cdf(3, 5)\n\n0.2650259152973616\n\n\nAnd the chance of observing at least 3 is equal to:\n\n1 - poisson.cdf(2, 5)\n\n0.8753479805169189\n\n\n(This works because you start with 1 (100%) exclude all options resulting in less than 3.)\nTo compute the probability of a certain range, just subtract the smaller value from the greater:\n\n# Probability of 2 to 4 falling stars:\npoisson.cdf(4, 5) - poisson.cdf(1, 5)\n\n0.40006560307069977\n\n\n(This works because you start with all possibilities up till 4 and then exclude all options resulting in less than 2.)\nThe probability of a value of \\(k \\leq 3\\) in Figure 2 is given by:\n\npoisson.cdf(3, 3)\n\n0.6472318887822313\n\n\n\n\n\n\n\n\n\n\nUp till what point does the probability equal at least \\(p\\)?\n\n\n\n\n\nFigure 3: Quantile function of a Poisson distribution with \\(\\lambda= 3\\).\n\n\n\n\nMathematical formula:\n\\[\\begin{align}\n  \\tag{3}\\label{qpois}\n  \\text{no simple closed form}\n\\end{align}\\]\nCode:\n\nRPython\n\n\n\nqpois(probability, lambda)\n\n\n\n\nfrom scipy.stats import poisson\npoisson.ppf(probability, mu)\n\nNote: mu refers to the mean (\\(\\lambda\\)).\n\n\n\n\n\n\n\n\n\nExplanation\n\n\n\n\n\nThe quantile function is the inverse of the cumulative distribution function. It tells you up till what point the probability equals \\(p\\). To obtain the opposite—the point after which the probability equals a certain value, simply compute the quantile of \\(1\\) minus the probability (see the examples below).\nThere exists no simple expression for the quantile function of the Poisson distribution, but it can be computed anyway using a numeric search. If you are interested how this works, I explain it below.\n\n\nIf no simple closed form exists, how is it calculated?\n\nThe quantile function answers the opposite of the cumulative distribution function:\n\nCumulative distribution function: What is the probability \\(p\\) up till a certain count \\(k\\)?\nQuantile function: What is the count \\(k\\) that has a probability of at least \\(p\\)?\n\nThis should give you a hint of how a numeric search might work: If you don’t have a quantile function, you can simply try plugging different \\(k\\)s in the cumulative distribution until you find the right answer.\nSuppose that in Figure 3, you need to know the value for \\(k\\) that has a total probability of at least 80%:\n\n# The real answer:\nqpois(0.8, lambda = 3)\n\n[1] 4\n\n# A numeric search using the CDF:\nk &lt;- 0\np &lt;- ppois(k, lambda = 3)\nwhile(p &lt; 0.8){\n  k &lt;- k + 1\n  p &lt;- ppois(k, lambda = 3)\n}\nprint(k)\n\n[1] 4\n\n\nThe search stops at \\(k=4\\), because that is the first instance of a probability of at least 80%.\nThere are more efficient ways to go about this, but this is the underlying principle of R’s qpois and Python’s poisson.ppf.\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n\n\nRPython\n\n\nIf there are on average 5 falling stars each night, what is the largest number of falling stars for 95% of all nights?\n\nqpois(0.95, lambda = 5)\n\n[1] 9\n\n\nWithin what range are the number of falling stars on 95% of all nights?\n\nqpois(c(0.025, 0.975), lambda = 5)\n\n[1]  1 10\n\n\nApparently, 95% of nights would have 1–10 falling stars if the average were 5.\n\n\nIf there are on average 5 falling stars each night, what is the largest number of falling stars for 95% of all nights?\n\nfrom scipy.stats import poisson\npoisson.ppf(0.95, 5)\n\n9.0\n\n\nWithin what range are the number of falling stars on 95% of all nights?\n\nresult = poisson.ppf([0.025, 0.975], 5)\nprint(result)\n\n[ 1. 10.]\n\n\nApparently, 95% of nights would have 1–10 falling stars if the average were 5."
  },
  {
    "objectID": "poissondistribution.html#quiz",
    "href": "poissondistribution.html#quiz",
    "title": " Poisson Distribution",
    "section": "Quiz",
    "text": "Quiz\n\nQuestion 1Answer\n\n\nUsing the following code, you can plot the probability mass function of Poisson distribution with \\(\\lambda = 2\\):\n\nRPython\n\n\n\nx &lt;- 0:20\ny &lt;- dpois(x, 2)\nplot(y ~ x, type = \"h\", lwd = 4, lend = 1, las = 1)\n\n\n\n\n\n\n\n\n\n\nTry it in browser\n\n\n\n\n\nLoading\n  webR...\n\n\n  \n\n\n(This is an experimental feature. I recommend just running RStudio in the background while reading this book, but perhaps this can be useful if you are reading this on your phone for example… Feedback is welcome!)\n\n\n\n\n\n\nimport numpy as np\nfrom scipy.stats import poisson\nimport matplotlib.pyplot as plt\n\nx = np.arange(0, 21)\ny = poisson.pmf(x, 2)\nplt.bar(x, y);\nplt.show()\n\n\n\n\n\n\n\nThis probability distribution is:\n\nContinuous / discrete\nLeft skewed / symmetric / right skewed\n\nTry increasing the rate \\(\\lambda\\) and plotting the result. The distribution is now:\n\nMore / less skewed\n\nDo you understand why this is the case?\n\n\n\n\n\n\nHint\n\n\n\n\n\nSee the video on probability distributions  (9:12).\n\n\n\n\n\nA Poisson distribution with \\(\\lambda = 2\\) is:\n\n\n\n\n\n\nContinuous / discrete (Only certain values are possible)\nLeft skewed / symmetric / right skewed (The right tail is longer)\n\nIf you increase the rate \\(\\lambda\\), the distribution is now:\n\nMore / less skewed (see Figure 4)\n\n\n\n\n\n\n\nExplanation\n\n\n\n\n\nCounts cannot be negative. If lambda is close to zero, the probability on the left side of the distribution gets pressed up into the side. If you increase lambda, you move further away from zero, so this skew becomes less apparent:\n\n\n\n\n\nFigure 4: Increasing \\(\\lambda\\) results in a more symmetric distribution.\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2Answer\n\n\nPerhaps the most commonly used laboratory animal is the mouse species C56BL/6. A study from 2015 estimated its litter size1 to be approximately 7.5.[1]\nAssuming litter size can be reasonably approximated with a Poisson distribution, what would be the chance of a litter size of at least 3?\n\n\n\n\n\nC56BL/6 mouse. Source: jax.org/strain/003548\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the code from exercise 1 to plot the distribution. This is the best way to get a feel for it.\n\n\n\n\n\nNow you can already see what would be a plausible answer: Almost all litter sizes are 3 or larger, so this should be well over 95%.\nThe question can be answered with the cumulative distribution function, see the explanation for details.\n\n\n\n\n\n\n\n\n\nTry it in browser\n\n\n\n\n\nLoading\n  webR...\n\n\n  \n\n\n(This is an experimental feature. I recommend just running RStudio in the background while reading this book, but perhaps this can be useful if you are reading this on your phone for example… Feedback is welcome!)\n\n\n\n\n\nUse the complement2 of the cumulative distribution function:\n\nRPython\n\n\n\n1 - ppois(2, 7.5)\n\n[1] 0.9797433\n\n\n\n\n\nfrom scipy.stats import poisson\n1 - poisson.cdf(2, 7.5)\n\n0.9797432849433356\n\n\n\n\n\nIf a Poisson distribution is a reasonable approximation, only about 2% of litters should be expected to have less than 3 mice.\n\n\n\n\nQuestion 3Answer\n\n\nContinuing on question 2, within what values would you expect 80% of all litter sizes to be?\n\n\n\n\n\n\nHint\n\n\n\n\n\nThis can be answered with the quantile function.\n\n\n\n\n\n\n\n\n\nTry it in browser\n\n\n\n\n\nLoading\n  webR...\n\n\n  \n\n\n(This is an experimental feature. I recommend just running RStudio in the background while reading this book, but perhaps this can be useful if you are reading this on your phone for example… Feedback is welcome!)\n\n\n\n\n\n80% lies between the lowest 10% and highest 90%:\n\nRPython\n\n\n\nqpois(c(0.1, 0.9), lambda = 7.5)\n\n[1]  4 11\n\n\n\n\n\nfrom scipy.stats import poisson\nresult = poisson.ppf([0.1, 0.9], 7.5)\nprint(result)\n\n[ 4. 11.]\n\n\n\n\n\nIf a Poisson distribution is a reasonable approximation, the answer is 4–11 mice.\n\n\n\n\n\n\n\n[1] J. B. Finlay, X. Liu, R. W. Ermel, and T. W. Adamson, “Maternal Weight Gain as a Predictor of Litter Size in Swiss Webster, C57BL/6J, and BALB/cJ mice,” J Am Assoc Lab Anim Sci, vol. 54, no. 6, pp. 694–699, Nov. 2015."
  },
  {
    "objectID": "poissondistribution.html#footnotes",
    "href": "poissondistribution.html#footnotes",
    "title": " Poisson Distribution",
    "section": "",
    "text": "The number of offspring born at one time.↩︎\nThe cumulative distribution function gives you the left tail of the distribution. Hence, 1 minus the function gives you the right tail. This is called the complement.↩︎"
  },
  {
    "objectID": "binomialdistribution.html#properties",
    "href": "binomialdistribution.html#properties",
    "title": " Binomial Distribution",
    "section": "Properties",
    "text": "Properties\n\nProbability mass functionCumulative distribution functionQuantile function\n\n\nWhat is the probability of a given count \\(k\\)?\n\n\n\n\n\nFigure 1: Probability mass function of a binomial distribution with \\(n = 10\\) trials and \\(p = 0.75\\).\n\n\n\n\nMathematical formula:\n\\[\\begin{align}\n  \\tag{1}\\label{dpois}\n  \\binom{n}{k} p^k (1 - p)^{n - k}\n\\end{align}\\]\nCode:\n\nRPython\n\n\n\ndbinom(k, n, p)\n\n\n\n\nfrom scipy.stats import binom\nbinom.pmf(k, n, p)\n\n\n\n\n\n\n\n\n\n\nSymbols\n\n\n\n\n\n\n\\(n\\): The number of trials\n\\(k\\): The number of successes\n\\(p\\): The probability of success\n\\(x \\choose y\\): The binomial coefficient\n\n\n\n\n\n\n\n\n\n\nExplanation\n\n\n\n\n\nThe binomial distribution is a discrete probability distribution. There are only \\(n\\) possible values for the outcome (\\(0, 1, 2, \\dots, n\\)), but not \\(1.5\\) or \\(2.01\\). Hence, you can directly read probabilities off the \\(y\\)-axis in Figure 1.\nThe correct term for a probability function of a discrete distribution is a probability mass function, though it is common in literature to see people call both discrete and continuous versions a probability density function.\nNote that while it looks like the probability is zero at \\(k = 2\\) or less in Figure 1, it is never exactly zero within the range of possible values. The probability is just too small to be visible in the figure.\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nRPython\n\n\nThe probability of \\(k = 4\\) in Figure 1 is:\n\ndbinom(4, 10, 0.75)\n\n[1] 0.016222\n\n\n\n\nThe probability of \\(k = 4\\) in Figure 1 is:\n\nfrom scipy.stats import binom\nbinom.pmf(4, 10, 0.75)\n\n0.016222000122070295\n\n\n\n\n\n\n\n\n\n\nWhat is the probability up till a given point \\(k\\)?\n\n\n\n\n\nFigure 2: Cumulative distribution function of a binomial distribution with \\(n = 10\\) trials and \\(p = 0.75\\).\n\n\n\n\nMathematical formula:\n\\[\\begin{align}\n  \\tag{2}\\label{ppois}\n  \\sum_{i = 0}^k \\binom{n}{i} p^i (1 - p)^{n - i}\n\\end{align}\\]\nCode:\n\nRPython\n\n\n\npbinom(k, n, p)\n\n\n\n\nfrom scipy.stats import binom\nbinom.cdf(k, n, p)\n\nNote: loc refers to the mean, scale to the standard deviation.\n\n\n\n\n\n\n\n\n\nSymbols\n\n\n\n\n\n\n\\(n\\): The number of trials\n\\(k\\): The number of successes\n\\(p\\): The probability of success\n\\(x \\choose y\\): The binomial coefficient\n\\(\\sum\\): Summation (compute for \\(0, 1, \\dots, k\\) and add all results)\n\n\n\n\n\n\n\n\n\n\nExplanation\n\n\n\n\n\nThe cumulative distribution function (CDF) is the sum of the probability mass function up till a given point \\(k\\). It can be used to calculate the probability of the number of successes being no greater than a certain value. To obtain the opposite—the chance of an observation greater than a certain value, simply use \\(1\\) minus the CDF. Finally, to compute the chance of being within a certain range, you can subtract the CDF of the larger value, from the CDF of the smaller value (see the examples below).\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n\n\nRPython\n\n\nIf you randomly guess multiple choice questions with 4 options on a 10-question exam, the chance of failing (\\(\\leq 5\\) questions right) is:\n\npbinom(5, 10, 1/4)\n\n[1] 0.9802723\n\n\nAnd the chance of at least 5 right is equal to:\n\n1 - pbinom(4, 10, 1/4)\n\n[1] 0.07812691\n\n\n(This works because you start with 1 (100%) exclude all options resulting in less than 5.)\nTo compute the probability of a certain range, just subtract the smaller value from the greater:\n\n# Probability of 5 to 7 questions right:\npbinom(7, 10, 1/4) - pbinom(4, 10, 1/4)\n\n[1] 0.07771111\n\n\n(This works because you start with all possibilities up till 7 and then exclude all options resulting in less than 5.)\nThe probability of a value of \\(k \\leq 5\\) in Figure 2 is given by:\n\npbinom(5, 10, 0.75)\n\n[1] 0.07812691\n\n\n\n\nIf you randomly guess multiple choice questions with 4 options on a 10-question exam, the chance of failing (\\(\\leq 5\\) questions right) is:\n\nfrom scipy.stats import binom\nbinom.cdf(5, 10, 1/4)\n\n0.9802722930908203\n\n\nAnd the chance of at least 5 right is equal to:\n\n1 - binom.cdf(4, 10, 1/4)\n\n0.07812690734863281\n\n\n(This works because you start with 1 (100%) exclude all options resulting in less than 5.)\nTo compute the probability of a certain range, just subtract the smaller value from the greater:\n\n# Probability of 5 to 7 questions right:\nbinom.cdf(7, 10, 1/4) - binom.cdf(4, 10, 1/4)\n\n0.07771110534667969\n\n\n(This works because you start with all possibilities up till 7 and then exclude all options resulting in less than 5.)\nThe probability of a value of \\(k \\leq 5\\) in Figure 2 is given by:\n\nbinom.cdf(5, 10, 0.75)\n\n0.07812690734863281\n\n\n\n\n\n\n\n\n\n\nUp till what point does the probability equal at least \\(p\\)?\n\n\n\n\n\nFigure 3: Quantile function of a binomial distribution with \\(n = 10\\) trials and \\(p = 0.75\\).\n\n\n\n\nMathematical formula:\n\\[\\begin{align}\n  \\tag{3}\\label{qbinom}\n  \\text{no simple closed form}\n\\end{align}\\]\nCode:\n\nRPython\n\n\n\nqbinom(probability, n, p)\n\n\n\n\nfrom scipy.stats import binom\nbinom.ppf(probability, n, p)\n\n\n\n\n\n\n\n\n\n\nExplanation\n\n\n\n\n\nThe quantile function is the inverse of the cumulative distribution function. It tells you up till what point the probability equals \\(p\\). To obtain the opposite—the point after which the probability equals a certain value, simply compute the quantile of \\(1\\) minus the probability (see the examples below).\nThere exists no simple expression for the quantile function of the binomial distribution, but it can be computed anyway using a numeric search. If you are interested how this works, I explain it below.\n\n\nIf no simple closed form exists, how is it calculated?\n\nThe quantile function answers the opposite of the cumulative distribution function:\n\nCumulative distribution function: What is the probability \\(p\\) up till a certain number of successes \\(k\\)?\nQuantile function: What is the number of successes \\(k\\) that has a probability of at least \\(p\\)?\n\nThis should give you a hint of how a numeric search might work: If you don’t have a quantile function, you can simply try plugging different \\(k\\)s in the cumulative distribution until you find the right answer.\nSuppose that in Figure 3, you need to know the value for \\(k\\) that has a total probability of at least 80%:\n\n# The real answer:\nqbinom(0.8, 10, 0.75)\n\n[1] 9\n\n# A numeric search using the CDF:\nk &lt;- 0\np &lt;- pbinom(k, 10, 0.75)\nwhile(p &lt; 0.8){\n  k &lt;- k + 1\n  p &lt;- pbinom(k, 10, 0.75)\n}\nprint(k)\n\n[1] 9\n\n\nThe search stops at \\(k=9\\), because that is the first instance of a probability of at least 80%.\nThere are more efficient ways to go about this, but this is the underlying principle of R’s qbinom and Python’s binom.ppf.\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n\n\nRPython\n\n\nIf all students randomly guess all 4-choice questions on a 10-question exam, what is the highest number of correct questions for 95% of students?\n\nqbinom(0.95, 10, 1/4)\n\n[1] 5\n\n\nWithin what range are the number of correct questions for 95% of all students?\n\nqbinom(c(0.025, 0.975), 10, 1/4)\n\n[1] 0 5\n\n\nApparently, 95% of students would answer 0–5 questions right.\n\n\nIf all students randomly guess all 4-choice questions on a 10-question exam, what is the highest number of correct questions for 95% of students?\n\nfrom scipy.stats import binom\nbinom.ppf(0.95, 10, 1/4)\n\n5.0\n\n\nWithin what range are the number of correct questions for 95% of all students?\n\nresult = binom.ppf([0.025, 0.975], 10, 1/4)\nprint(result)\n\n[0. 5.]\n\n\nApparently, 95% of students would answer 0–5 questions right."
  },
  {
    "objectID": "binomialdistribution.html#quiz",
    "href": "binomialdistribution.html#quiz",
    "title": " Binomial Distribution",
    "section": "Quiz",
    "text": "Quiz\n\nQuestion 1Answer\n\n\nUsing the following code, you can plot the probability mass function of binomial distribution with \\(n = 10\\) and \\(p = 0.8\\):\n\nRPython\n\n\n\nn &lt;- 10\np &lt;- 0.8\nx &lt;- 0:n\ny &lt;- dbinom(x, n, p)\nplot(y ~ x, type = \"h\", lwd = 10, lend = 1, las = 1)\n\n\n\n\n\n\n\n\n\n\nTry it in browser\n\n\n\n\n\nLoading\n  webR...\n\n\n  \n\n\n(This is an experimental feature. I recommend just running RStudio in the background while reading this book, but perhaps this can be useful if you are reading this on your phone for example… Feedback is welcome!)\n\n\n\n\n\n\nimport numpy as np\nfrom scipy.stats import binom\nimport matplotlib.pyplot as plt\n\nn = 10\np = 0.8\nx = np.arange(0, n + 1)\ny = binom.pmf(x, n, p)\nplt.bar(x, y);\nplt.show()\n\n\n\n\n\n\n\nThis probability distribution is:\n\nContinuous / discrete\nLeft skewed / symmetric / right skewed\n\nTry increasing the number of trials (\\(n\\)) and plotting the result. The distribution is now:\n\nMore / less skewed\n\nDo you understand why this is the case?\n\n\n\n\n\n\nHint\n\n\n\n\n\nSee the video on probability distributions  (13:39).\n\n\n\n\n\nA binomial distribution with \\(n = 10\\) and \\(p = 0.8\\) is:\n\n\n\n\n\n\nContinuous / discrete (Only certain values are possible)\nLeft skewed / symmetric / right skewed (The left tail is longer)\n\nIf you increase the number of trials (\\(n\\)), the distribution is now:\n\nMore / less skewed (see Figure 4)\n\n\n\n\n\n\n\nExplanation\n\n\n\n\n\nThe number of successes cannot be negative, nor larger than the number of trials. If \\(p\\) is close to zero or one, the remaining probability gets pressed up into the side. If you increase \\(n\\), or move \\(p\\) closer to a half, you move further away from zero, so this skew becomes less apparent:\n\n\n\n\n\nFigure 4: Increasing \\(n\\), or moving \\(p\\) closer to \\(0.5\\) results in a more symmetric distribution.\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2Answer\n\n\nFor a field experiment, students go out and plant 30 flower seeds. The packaging mentions 1 in 4 seeds can be expected to germinate.\nWhat is the chance of at least 10 seeds yielding flowers?\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the code from exercise 1 to plot the distribution. This is the best way to get a feel for it.\n\n\n\n\n\nThe question can be answered with the cumulative distribution function, see the explanation for details.\n\n\n\n\n\n\n\n\n\nTry it in browser\n\n\n\n\n\nLoading\n  webR...\n\n\n  \n\n\n(This is an experimental feature. I recommend just running RStudio in the background while reading this book, but perhaps this can be useful if you are reading this on your phone for example… Feedback is welcome!)\n\n\n\n\n\nUse the complement1 of the cumulative distribution function:\n\nRPython\n\n\n\n1 - pbinom(9, 30, 1/4)\n\n[1] 0.1965934\n\n\n\n\n\nfrom scipy.stats import binom\n1 - binom.cdf(9, 30, 1/4)\n\n0.19659336305048913\n\n\n\n\n\nIf a binomial distribution is a reasonable approximation, there is an approximately 1 in 5 chance of 10 or more flowers.\n\n\n\n\nQuestion 3Answer\n\n\nContinuing on question 2, what is the smallest number of flowers the students should expect, with a 5% chance of happening?\n\n\n\n\n\n\nHint\n\n\n\n\n\nThis can be answered with the quantile function.\n\n\n\n\n\n\n\n\n\nTry it in browser\n\n\n\n\n\nLoading\n  webR...\n\n\n  \n\n\n(This is an experimental feature. I recommend just running RStudio in the background while reading this book, but perhaps this can be useful if you are reading this on your phone for example… Feedback is welcome!)\n\n\n\n\n\nThe smallest 5% can be found with the quantile function:\n\nRPython\n\n\n\nqbinom(0.05, 30, 1/4)\n\n[1] 4\n\n\n\n\n\nfrom scipy.stats import binom\nbinom.ppf(0.05, 30, 1/4)\n\n4.0\n\n\n\n\n\nThe students can expect at least 4 flowers."
  },
  {
    "objectID": "binomialdistribution.html#footnotes",
    "href": "binomialdistribution.html#footnotes",
    "title": " Binomial Distribution",
    "section": "",
    "text": "The cumulative distribution function gives you the left tail of the distribution. Hence, 1 minus the function gives you the right tail. This is called the complement.↩︎"
  },
  {
    "objectID": "regressionanalysis.html",
    "href": "regressionanalysis.html",
    "title": " Regression Analysis",
    "section": "",
    "text": "This chapter introduces the basics of regression analysis, which we will then use for estimation, comparisons, and prediction."
  },
  {
    "objectID": "simplelinearregression.html#slm-lecture",
    "href": "simplelinearregression.html#slm-lecture",
    "title": "1  Simple Linear Regression",
    "section": "1.1 Lecture",
    "text": "1.1 Lecture\n\n1. Introduction2. Assumptions3. Regression Table4. Diagnostics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Open playlist in a new tab"
  },
  {
    "objectID": "simplelinearregression.html#summary",
    "href": "simplelinearregression.html#summary",
    "title": "1  Simple Linear Regression",
    "section": "1.2 Summary",
    "text": "1.2 Summary\n\n\n\n\n\nFigure 1.1: Example of a simple linear model. (Height and weight of 50 individuals.)\n\n\n\n\nMathematical formula:\n\\[\\begin{align}\n  y &= \\beta_0 + \\beta_1 \\cdot x + \\epsilon \\\\ \\tag{1}\\label{slm}\\\\\n  \\epsilon &\\sim \\mathcal{N}(0, \\, \\sigma^2)\n\\end{align}\\]\nCode:\n\nRPython\n\n\n\nmodel &lt;- lm(y ~ x, data)\n\n\n\n\nimport statsmodels.api as sm\nmodel = sm.OLS(y, x).fit()\n\nNote: You have to include an intercept manually using x = sm.add_constant(x).\n\n\n\n\n\n\n\n\n\nDefinitions\n\n\n\n\n\n\nresponse variable\n\nThe outcome, denoted by \\(y\\).\n\n\n\\(\\hat{y}\\) is the predicted outcome by the model, also called the ‘fitted value.’\n\nexplanatory variable\n\nThe variable the outcome is regressed on, denoted by \\(x\\).\n\nintercept\n\nValue of the outcome (\\(y\\)), when the explanatory variable (\\(x\\)) is zero.\n\n\n\\(\\beta_0\\) is the theoretical value.\n\n\n\\(\\hat{\\beta}_0\\) is its estimate.\n\nslope\n\nChange in the outcome (\\(y\\)), when the explanatory variable (\\(x\\)) increases by one.\n\n\n\\(\\beta_1\\) is the theoretical value.\n\n\n\\(\\hat{\\beta}_1\\) is its estimate.\n\nerror\n\nDifference between the true population values of the outcome and those predicted by the true model.\n\n\n\\(\\epsilon\\) is the true error.\n\nresidual\n\nDifference between the sample values of the outcome and those predicted by the estimated model.\n\n\n\\(\\hat{\\epsilon}\\) is sometimes used to denote the residual, an ‘estimate’ of the error.\n\nnormal distribution\n\nSee  Normal Distribution.\n\n\n\\(\\epsilon \\sim \\mathcal{N}(0, \\, \\sigma^2)\\) means: The error follows a normal distribution with mean \\(0\\) and variance \\(\\sigma^2\\) (some unknown variance).\n\n\n\n\n\n\n\n\n\n\n\nExplanation\n\n\n\n\n\nSimple linear regression models the relationship between two continuous1 variables, using an intercept and a slope:\n\n\n\n\n\nThe intercept is the value of \\(y\\), when \\(x=0\\). The slope is the change in \\(y\\), when \\(x\\) increases by \\(1\\).\n\n\n\n\nAs you can see in figure Figure 1.1, a model is only an approximation and there will always be individual differences from the ‘average’ trend that has been estimated. These differences are called the residuals:\n\n\n\n\n\nFigure 1.2: Residual of a simple linear model.\n\n\n\n\n\nThe intercept and slope are estimated by minimizing the sum of squared residuals, a technique called ordinary least squares.2\n\n\n\n\n\nFigure 1.3: Squared residuals give porportionally more weight to farther observations.\n\n\n\n\n\nThe results of a linear regression are summarized in a regression table, which provides insight into the parameter estimates, their uncertainty, and some basic measures of how well the model fits the data:\n\nRPython\n\n\n\nDF &lt;- read.csv(\"data/example_data_simple_linear_regression.csv\")\nLM &lt;- lm(weight ~ height, data = DF)\nsummary(LM)\n\n\nCall:\nlm(formula = weight ~ height, data = DF)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-21.486 -11.962  -2.670   6.326  39.192 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -57.4496    32.1694  -1.786 0.084960 .  \nheight        0.8025     0.1859   4.318 0.000178 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.82 on 28 degrees of freedom\nMultiple R-squared:  0.3997,    Adjusted R-squared:  0.3783 \nF-statistic: 18.64 on 1 and 28 DF,  p-value: 0.0001783\n\n\n\n\n\nimport pandas as pd\nimport statsmodels.api as sm\nDF = pd.read_csv(\"data/example_data_simple_linear_regression.csv\")\nLM = sm.OLS(DF['height'], sm.add_constant(DF['weight'])).fit()\nprint(LM.summary2())\n\n                 Results: Ordinary least squares\n=================================================================\nModel:              OLS              Adj. R-squared:     0.378   \nDependent Variable: height           AIC:                234.5127\nDate:               2023-09-03 17:57 BIC:                237.3151\nNo. Observations:   30               Log-Likelihood:     -115.26 \nDf Model:           1                F-statistic:        18.64   \nDf Residuals:       28               Prob (F-statistic): 0.000178\nR-squared:          0.400            Scale:              136.30  \n------------------------------------------------------------------\n            Coef.    Std.Err.     t     P&gt;|t|    [0.025    0.975] \n------------------------------------------------------------------\nconst      132.1493    9.5791  13.7955  0.0000  112.5273  151.7712\nweight       0.4981    0.1154   4.3178  0.0002    0.2618    0.7344\n-----------------------------------------------------------------\nOmnibus:              0.339        Durbin-Watson:           2.521\nProb(Omnibus):        0.844        Jarque-Bera (JB):        0.508\nSkew:                 0.156        Prob(JB):                0.776\nKurtosis:             2.444        Condition No.:           373  \n=================================================================\nNotes:\n[1] Standard Errors assume that the covariance matrix of the\nerrors is correctly specified.\n\n\n\n\n\n\nA simple linear model makes several key assumptions which are required for valid inference:\n\nThe observations are independent.\nThe relationship between the outcome and the explanatory variable is linear.\nThe error follows a normal distribution.\nThe error has a constant variance along the regression line.\n\nAnd while not really an assumption, also important:\n\nThere are no influential outliers with unreasonably large influence on the estimates.\n\nIndependent measurements must be apparent from the study design. The rest can be checked through visual diagnostics. A quick visual summary of what each plot is used for:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nThis is example uses the iris data set. To use your own data, have a look here ).\nSuppose we want to estimate the relationship between the length of a flower sepals and the width of its petals. We’ll consider sepal length to be the outcome in this example.\n\nRPython\n\n\n\nPlot the data\n\n\nplot(Sepal.Length ~ Petal.Width, iris, \n     bty = \"n\", pch = 19, las = 1,\n     xlab = \"Petal width (mm)\", \n     ylab = \"Sepal length (mm)\")\n\n\n\n\n(To change the aesthetics, search for options online, or in the help files of plot and par.)\nThere appears to be a positive correlation between the two variables: Flowers with wider petals have longer sepals, on average.\n\n\nFit the model (this does not print any output)\n\n\nLM &lt;- lm(Sepal.Length ~ Petal.Width, iris)\n\n\n\nWhat are the estimated intercept and slope?\n\n\ncoef(LM)\n\n(Intercept) Petal.Width \n  4.7776294   0.8885803 \n\n\nIn this case, \\(\\hat{\\beta}_0 = 4.8\\) and \\(\\hat{\\beta}_1 = 0.9\\). So a flower with 1 mm wider petals, has on average 0.9 mm longer sepals.\n\n\nPerform visual diagnostics\n\n\nrequire(\"car\")         # Install if missing\npar(mfrow = c(2, 2))   # Plot in a 2x2 grid\nplot(LM, which = 1)    # Residuals vs fitted\nqqPlot(LM, reps = 1e4) # QQ-plot\nplot(LM, which = 3)    # Scale-location\nplot(LM, which = 5)    # Residuals vs leverage\npar(mfrow = c(1, 1))   # Restore the default\n\n\n\n\n\n\n\nResiduals vs fitted: There is no non-linear pattern at all. The red smoothing line is as good as flat.\nQQ-plot: The residuals quite closely follow the line representing a true normal distribution. There is a wiggle near the top right, but I would not find this cause for concern.3\nScale-location: The variance increases along the regression line, indicated by the clear upward trend. However, from the \\(y\\)-axis, we can see that this increase is small (\\(&lt;0.5\\)) and no cause for concern.4\nResiduals vs leverage: The boundaries for the Cook’s distance are not even visible in the plot, so there are no outliers.\n\nIn conclusion, no serious problems are revealed.\n\n\nPrint a regression table\n\n\nsummary(LM)\n\n\nCall:\nlm(formula = Sepal.Length ~ Petal.Width, data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.38822 -0.29358 -0.04393  0.26429  1.34521 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.77763    0.07293   65.51   &lt;2e-16 ***\nPetal.Width  0.88858    0.05137   17.30   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.478 on 148 degrees of freedom\nMultiple R-squared:  0.669, Adjusted R-squared:  0.6668 \nF-statistic: 299.2 on 1 and 148 DF,  p-value: &lt; 2.2e-16\n\n\nThere is a significant relationship between petal width and sepal length (\\(\\hat{\\beta} = 0.889\\), \\(p &lt; 2 \\cdot 10^{-16}\\)). This model explains 66.9% of the variance in sepal length.\nThe model can be plotted as explained in the video.\n\n\n…"
  },
  {
    "objectID": "simplelinearregression.html#quiz",
    "href": "simplelinearregression.html#quiz",
    "title": "1  Simple Linear Regression",
    "section": "1.3 Quiz",
    "text": "1.3 Quiz\n\nQuestion 1Answer\n\n\nWhich of the following is implicitly assumed in simple linear regression? (choose one)\nA. The data are normally distributed\nB. The outcome is normally distributed\nC. The residuals are normally distributed\nD. The response variable is normally distributed\n\n\n\n\n\n\nHint\n\n\n\n\n\nSee equation \\(\\eqref{slm}\\).\n\n\n\n\n\nWhich of the following is implicitly assumed in simple linear regression? (choose one)\nA. The data are normally distributed\nB. The outcome is normally distributed\nC. The residuals are normally distributed\nD. The response variable is normally distributed\n\n\n\n\n\n\nLearn why\n\n\n\n\n\nThe normality assumption is about the error, the vertical distances along the regression line. Our ‘estimate’ of the error is called the residual.\nSimple linear regression makes no assumptions about your entire data set, nor your outcome variable. Response variable is just a synonym for outcome.\n\nAnother correct way to phrase the assumption is that the outcome is conditionally normally distributed. The condition here is the value of the explanatory variable. Namely, we can rewrite equation \\(\\eqref{slm}\\) as follows:\n\\[\\begin{align}\n  y \\sim \\mathcal{N}(\\beta_0 + \\beta_1 \\cdot x, \\, \\sigma^2)\n\\end{align}\\]\nThis says: “The outcome follows a normal distribution with a mean that depends on the value of \\(x\\).”\nDon’t worry if you do not fully understand this yet, as it will be covered at length in the next chapter.\n\n\n\n\n\n\n\nQuestion 2Answer\n\n\nIn the context of simple linear regression, a confidence interval shows:\nA. Where to expect current observations\nB. Where to expect future observations\nC. A plausible range of values for the intercept and slope\nD. A 95% confidence interval has a 95% chance to contain the true value for the intercept and slope\n\n\n\n\n\n\nHint\n\n\n\n\n\nThis is mentioned in the  first video , and more thoroughly explained in a separate video:\n\n Open video in a new tab \n\n\n\n\n\nIn the context of simple linear regression, a confidence interval shows:\nA. Where to expect current observations\nB. Where to expect future observations\nC. A plausible range of values for the intercept and slope\nD. A 95% confidence interval has a 95% chance to contain the true intercept/slope\n\n\n\n\n\n\nLearn why\n\n\n\n\n\nA. You don’t need an interval for this, just look at the observations.\nB. This is the interpretation of a prediction interval.\nC. This is a loose interpretation of a confidence interval, I recommend using.5\nD. This is the interpretation of a Bayesian credible interval.\nFor a more in depth explanation, please refer to the video:\n\n Open video in a new tab \n\n\n\n\n\n\n\nQuestion 3Answer\n\n\nBelow is the output of a simple linear model:\n\n\n             Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)  4.817080  0.3375541 14.270542 5.668035e-07\nx           -2.016138  0.4487337 -4.492949 2.020849e-03\n\n\nWhich of the following best represents the model?\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nLook at the Estimate column.\n\n\n\n\n\nBelow is the output of a simple linear model:\n\n\n             Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)  4.817080  0.3375541 14.270542 5.668035e-07\nx           -2.016138  0.4487337 -4.492949 2.020849e-03\n\n\nOption C best represents the model:\n\n\n\n\n\n\n\n\n\n\n\nLearn why\n\n\n\n\n\nFilling in the estimates, the fitted model is:\n\\[\\begin{align}\n  \\hat{y} = \\texttt{4.817} + \\texttt{-2.016} \\cdot x\n\\end{align}\\]\nThe estimated slope is about -2, so the line should go down by 2 for every unit increase in \\(x\\).\n\nThe intercept is the value of the outcome, if we set the explanatory variable to zero. You may have learned before that the intercept is where you ‘cross the \\(y\\)-axis,’ but the figures are (intentionally) misleading to teach you an important nuance: The \\(y\\)-axis does not have to start at \\(0\\).\nBelow is the same figure, with a vertical line drawn at \\(x=0\\):\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 4Answer\n\n\nContinuing on question 3, below is another part of the output of this model:\n\n\nResidual standard error: 1.051 on 8 degrees of freedom\nMultiple R-squared:  0.7162,    Adjusted R-squared:  0.6807 \nF-statistic: 20.19 on 1 and 8 DF,  p-value: 0.002021\n\n\nAnswer the following questions:\n\nThis model explains a significant/non-significant part of the variance in the outcome.\nThis model explains …% of the variance in the outcome.\nThe original sample size was … and the model uses … degrees of freedom.\nThe residuals have a standard deviation of ….\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThis is explained in the  third video .\n\n\n\n\n\nContinuing on question 3, below is another part of the output of the model:\n\n\nResidual standard error: 1.051 on 8 degrees of freedom\nMultiple R-squared:  0.7162,    Adjusted R-squared:  0.6807 \nF-statistic: 20.19 on 1 and 8 DF,  p-value: 0.002021\n\n\nAnswer the following questions:\n\nThis model explains a significant/non-significant part of the variance in the outcome.\nThis model explains 71.6% of the variance in the outcome.\nThe original sample size was 10 and the model uses 2 degrees of freedom.\nThe residuals have a standard deviation of 1.05.\n\n\n\n\n\n\n\nLearn why\n\n\n\n\n\n\nThe \\(F\\)-test at the bottom compares the original spread in the outcome to the spread that remains around the regression line after fitting the model (i.e. the residual variance). Its \\(p\\)-value is \\(0.002\\), which would be considered significant at most commonly used levels of significance, like \\(\\alpha = 0.05\\), or \\(\\alpha = 0.01\\).\nVariance explained is the interpretation of \\(R^2\\).\nThe residual degrees of freedom are 8 according to the output. A simple linear model consists of an intercept and a slope, so the model uses 2 degrees of freedom. That means the original sample size was \\(8 + 2 = 10\\).\nThe spread around the regression line is printed with the incorrect wording residual standard error: 1.051, as mentioned in the video. It should actually say residual standard deviation: 1.051.\n\n\n\n\n\n\n\n\nQuestion 5Answer\n\n\nBelow are some diagnostic plots for the model fitted above. Explain what each plot is for and comment on the results:\n\n\n\n\n\n\nThe residuals vs fitted plot is used to assess the assumption of …. In this case it shows ….\nThe QQ-plot is used to assess the assumptions of …. In this case it shows ….\nThe scale-location plot is used to assess the assumption of …. In this case it shows ….\nThe residuals vs leverage plot is used to assess …. In this case it shows ….\n\nBased on these diagnostic plots, I conclude ….\n\n\n\n\n\n\nHint\n\n\n\n\n\nThis is covered in the  fourth video .\n\n\n\n\n\nBelow are some diagnostic plots for the model fitted above. Explain what each plot is for and comment on the results:\n\n\n\n\n\n\nThe residuals vs fitted plot is used to assess the assumption of linearity. In this case it shows a non-linear pattern, although that can be attributed to a single observation (4).\nThe QQ-plot is used to assess the assumptions of normality. In this case it shows left skew, indicated by the convex shape. However, the deviation from normality is within what is acceptable, according to the confidence band.\nThe scale-location plot is used to assess the assumption of constant variance. In this case it shows no consistent upward or downward trend in the variance along the regression line.\nThe residuals vs leverage plot is used to assess the presence and influence of outliers. In this case it shows a single outlying observation with high leverage (4).\n\nBased on these diagnostic plots, I conclude observation 4 has a large influence on the parameter estimates and must be checked for validity.\n\n\n\n\n\n\nLearn why\n\n\n\n\n\n…"
  },
  {
    "objectID": "simplelinearregression.html#footnotes",
    "href": "simplelinearregression.html#footnotes",
    "title": "1  Simple Linear Regression",
    "section": "",
    "text": "Continuous can take on any value within a range, like \\(1,\\, 1.01,\\, 1.001,\\, \\dots\\)\nDiscrete can only take on certain values, like \\(1,\\, 2,\\, 3,\\, \\dots\\)↩︎\nIt is called ordinary in contrast to the many adaptation that exist, like weighted least squares, partial least squares, etc.↩︎\nIf you are in doubt, you can use a test for deviation from normality:\n\nshapiro.test(resid(LM)) # Shapiro-Wilks test\n\n\n    Shapiro-Wilk normality test\n\ndata:  resid(LM)\nW = 0.98653, p-value = 0.1543\n\n\nWith \\(p = 0.154\\), no significant deviation from normality was detected.↩︎\nIf you are in doubt, you can use a test for deviation from constant variance:\n\nlibrary(\"lmtest\")\nbptest(LM) # Breusch-Pagan test\n\n\n    studentized Breusch-Pagan test\n\ndata:  LM\nBP = 12.357, df = 1, p-value = 0.0004393\n\n\nWith \\(p = 0.00044\\), significant non-constant variance was detected. However, from the scale-location plot we saw that the magnitude of the non-constant variance was too small to be of concern.↩︎\nPurists may object to this definition, but as shown in the end of the video on confidence intervals, this interpretation is often very reasonable for the types of models you will learn about in this course.↩︎"
  },
  {
    "objectID": "GLM.html#properties",
    "href": "GLM.html#properties",
    "title": "2  Generalized Linear Models",
    "section": "2.1 Properties",
    "text": "2.1 Properties\n\nNormalPoissonBinomial\n\n\n\n\n\n\n\nFigure 2.1: Example of a simple linear model. (Height and weight of 50 individuals.)\n\n\n\n\nMathematical formula:\n\nClassical notationConsistent with the rest\n\n\n\\[\\begin{align}\n  \\label{lm}\n  y &= \\beta_0 + \\beta_1 \\cdot x + \\epsilon \\\\ \\tag{1a}\\\\\n  \\epsilon &\\sim \\mathcal{N}(0, \\, \\sigma^2)\n\\end{align}\\]\n\n\n\\[\\begin{align}\n  \\label{glmnorm}\n  y &\\sim \\mathcal{N}(\\mu, \\, \\sigma^2)  \\\\ \\\\\n  \\mu &= \\eta\\tag{1b}\\\\ \\\\\n  \\eta &= \\beta_0 + \\beta_1 \\cdot x\n\\end{align}\\]\nOrdinary linear regression can also be written as a generalized linear model (GLM). This has no particular advantage in terms of analysis, but it may help you understand how this method relates to other GLMs.\nTry comparing this formula to that of Poisson and binomial regression: Essentially, we are estimating the location parameter of a distribution, and that parameter changes based on the value of the explanatory variable (\\(x\\)). The only difference with the other GLMs is which distribution we assume.\nThe middle equation for each of the three GLMs is the link function; how the location parameter of chosen distribution translates to the linear equation (\\(\\eta\\)). For a normal distribution, this is called the ‘identity’, which just multiplies everything by \\(1\\), such that the linear equation is the mean of the normal distribution.\n\n\n\nCode:\n\nRPython\n\n\n\nmodel &lt;- lm(y ~ x, data)\n\n\n\n\nimport statsmodels.api as sm\nmodel = sm.OLS(y, x).fit()\n\nNote: You have to include an intercept manually using x = sm.add_constant(x).\n\n\n\n\n\n\n\n\n\nSymbols & definitions\n\n\n\n\n\n\n\\(y\\) — response variable\n\nThe outcome.\n\nNormal distribution\n\nSee  Normal Distribution. \\(y \\sim \\mathcal{N}\\! \\left(\\mu, \\, \\sigma^2\\right)\\) means: The outcome follows a conditional2 normal distribution with mean \\(\\mu\\).\n\nlink function\n\nA function that stretches the possible range of values of mean of the outcome to \\(-\\infty\\) to \\(+\\infty\\), which is convenient for estimating the model parameters, as it allows us to use a modified version of least squares (iteratively reweighted least squares).\n\ncanonical link function\n\nThe one link function that translates the linear predictor to the mean of the outcome. For the Poisson distribution, this is the logarithm: \\(\\log(\\lambda)\\)\n\n\\(\\eta\\) — linear predictor\n\nThe function that describes the relationship between the explanatory variable and the (transformed) mean of the outcome.\n\n\\(x\\) — explanatory variable\n\nThe variable the outcome is regressed on.\n\n\\(\\beta_0\\) — intercept\n\nValue of the outcome (\\(y\\)), when the explanatory variable (\\(x\\)) is zero. \\(\\hat{\\beta}_0\\) (beta-hat) is its estimate.\n\n\\(\\beta_1\\) — slope\n\nChange in the outcome (\\(y\\)), when the explanatory variable (\\(x\\)) increases by one. \\(\\hat{\\beta}_1\\) (beta-hat) is its estimate.\n\n\n\n\n\n\n\n\n\n\n\nExplanation\n\n\n\n\n\n(…)\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nThis is example uses the iris data set. To use your own data, have a look here ).\nFitting an ordinary linear model can be done by fitting a generalized linear model, but it is good practice to use the simplest possible model/notation. Hence, the example here is merely meant to show that:\n\nAn ordinary linear model is a GLM, and hence…\n…a normal GLM will give you the same results as an ordinary linear model.\n\nSuppose we want to estimate the relationship between the Sepals and Petals of iris flowers. For this example we will consider the Sepal length to be the outcome.\n\nRPython\n\n\nCompare the output of lm with that of glm. Which parts overlap? Which parts are different and why?\n\nLM &lt;- lm(Sepal.Length ~ Petal.Length, data = iris)\nsummary(LM)\n\n\nCall:\nlm(formula = Sepal.Length ~ Petal.Length, data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.24675 -0.29657 -0.01515  0.27676  1.00269 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   4.30660    0.07839   54.94   &lt;2e-16 ***\nPetal.Length  0.40892    0.01889   21.65   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4071 on 148 degrees of freedom\nMultiple R-squared:   0.76, Adjusted R-squared:  0.7583 \nF-statistic: 468.6 on 1 and 148 DF,  p-value: &lt; 2.2e-16\n\n\n\nGLM &lt;- glm(Sepal.Length ~ Petal.Length, data = iris, family = \"gaussian\")\nsummary(GLM)\n\n\nCall:\nglm(formula = Sepal.Length ~ Petal.Length, family = \"gaussian\", \n    data = iris)\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   4.30660    0.07839   54.94   &lt;2e-16 ***\nPetal.Length  0.40892    0.01889   21.65   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.1657097)\n\n    Null deviance: 102.168  on 149  degrees of freedom\nResidual deviance:  24.525  on 148  degrees of freedom\nAIC: 160.04\n\nNumber of Fisher Scoring iterations: 2\n\n\n\n\nCompare the output of sm.OLS with that of sm.GLM. Which parts overlap? Which parts are different and why?\n\nimport pandas as pd\nimport statsmodels.api as sm\n\niris = sm.datasets.get_rdataset('iris').data\nX = iris['Petal.Length']\ny = iris['Sepal.Length']\nX = sm.add_constant(X) # Add an intercept\nmodel = sm.OLS(y, X).fit()\nprint(model.summary2())\n\n                 Results: Ordinary least squares\n=================================================================\nModel:              OLS              Adj. R-squared:     0.758   \nDependent Variable: Sepal.Length     AIC:                158.0404\nDate:               2023-09-03 17:57 BIC:                164.0617\nNo. Observations:   150              Log-Likelihood:     -77.020 \nDf Model:           1                F-statistic:        468.6   \nDf Residuals:       148              Prob (F-statistic): 1.04e-47\nR-squared:          0.760            Scale:              0.16571 \n------------------------------------------------------------------\n                 Coef.   Std.Err.     t     P&gt;|t|   [0.025  0.975]\n------------------------------------------------------------------\nconst            4.3066    0.0784  54.9389  0.0000  4.1517  4.4615\nPetal.Length     0.4089    0.0189  21.6460  0.0000  0.3716  0.4463\n-----------------------------------------------------------------\nOmnibus:              0.207        Durbin-Watson:           1.867\nProb(Omnibus):        0.902        Jarque-Bera (JB):        0.346\nSkew:                 0.069        Prob(JB):                0.841\nKurtosis:             2.809        Condition No.:           10   \n=================================================================\nNotes:\n[1] Standard Errors assume that the covariance matrix of the\nerrors is correctly specified.\n\n\n\nimport statsmodels.api as sm\nimport pandas as pd\nimport statsmodels.api as sm\n\niris = sm.datasets.get_rdataset('iris').data\nX = iris['Petal.Length']\ny = iris['Sepal.Length']\nX = sm.add_constant(X) # Add an intercept\nmodel = sm.GLM(y, X, family = sm.families.Gaussian()).fit()\nprint(model.summary2())\n\n              Results: Generalized linear model\n==============================================================\nModel:              GLM              AIC:            158.0404 \nLink Function:      Identity         BIC:            -717.0490\nDependent Variable: Sepal.Length     Log-Likelihood: -77.020  \nDate:               2023-09-03 17:57 LL-Null:        -311.30  \nNo. Observations:   150              Deviance:       24.525   \nDf Model:           1                Pearson chi2:   24.5     \nDf Residuals:       148              Scale:          0.16571  \nMethod:             IRLS                                      \n---------------------------------------------------------------\n              Coef.   Std.Err.     z     P&gt;|z|   [0.025  0.975]\n---------------------------------------------------------------\nconst         4.3066    0.0784  54.9389  0.0000  4.1530  4.4602\nPetal.Length  0.4089    0.0189  21.6460  0.0000  0.3719  0.4459\n==============================================================\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.2: Example of Poisson regression. (Apples per year as a function of tree age.)\n\n\n\n\nMathematical formula:\n\\[\\begin{align}\n  \\label{glmpois}\n  y &\\sim \\mathsf{Pois}(\\lambda)  \\\\ \\\\\n  \\tag{2}\\log(\\lambda) &= \\eta \\\\ \\\\\n  \\eta &= \\beta_0 + \\beta_1 \\cdot x\n\\end{align}\\]\nCode:\n\nRPython\n\n\n\nmodel &lt;- glm(y ~ x, data, family = \"poisson\")\n\n\n\n\nimport statsmodels.api as sm\nmodel = sm.GLM(y, X, family = sm.families.Poisson()).fit()\n\nNote: You have to include an intercept manually using X = sm.add_constant(X).\n\n\n\n\n\n\n\n\n\nSymbols & definitions\n\n\n\n\n\n\n\\(y\\) — response variable\n\nThe outcome.\n\nPoisson distribution\n\nSee  Poisson Distribution. \\(y \\sim \\mathsf{Pois}(\\lambda)\\) means: The outcome follows a conditional3 Poisson distribution with rate \\(\\lambda\\).\n\nlink function\n\nA function that stretches the possible range of values of mean of the outcome to \\(-\\infty\\) to \\(+\\infty\\), which is convenient for estimating the model parameters, as it allows us to use a modified version of least squares (iteratively reweighted least squares).\n\ncanonical link function\n\nThe one link function that translates the linear predictor to the mean of the outcome. For the Poisson distribution, this is the logarithm: \\(\\log(\\lambda)\\)\n\n\\(\\eta\\) — linear predictor\n\nThe function that describes the relationship between the explanatory variable and the (transformed) mean of the outcome.\n\n\\(x\\) — explanatory variable\n\nThe variable the outcome is regressed on.\n\n\\(\\beta_0\\) — intercept\n\nValue of the outcome (\\(y\\)), when the explanatory variable (\\(x\\)) is zero. \\(\\hat{\\beta}_0\\) (beta-hat) is its estimate.\n\n\\(\\beta_1\\) — slope\n\nChange in the outcome (\\(y\\)), when the explanatory variable (\\(x\\)) increases by one. \\(\\hat{\\beta}_1\\) (beta-hat) is its estimate.\n\n\n\n\n\n\n\n\n\n\n\nExplanation\n\n\n\n\n\n(…)\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nThis is example uses the ... data set. To use your own data, have a look here ).\nSuppose we want to estimate the relationship between the … and ….\n\nRPython\n\n\n…\n\n\n…\n\n\n\n\n\n\n\n\n\nExample: RatioExample: Binary\n\n\n\n\n\n\n\nFigure 2.3: Example of logistic regression. (Correct questions as a function of studying time.)\n\n\n\n\nMathematical formula:\n\\[\\begin{align}\n  \\label{glmbinom}\n  y &\\sim \\mathsf{Binom}(n, \\, p)  \\\\ \\\\\n  \\tag{3a}\\log \\left( \\frac{p}{1-p} \\right) &= \\eta \\\\ \\\\\n  \\eta &= \\beta_0 + \\beta_1 \\cdot x\n\\end{align}\\]\nCode:\n\nRPython\n\n\n\nmodel &lt;- glm(cbind(success, failure) ~ x, data, family = \"binomial\")\n\n\n\n\nimport statsmodels.api as sm\nmodel = sm.GLM(y, X, family = sm.families.Binomial()).fit()\n\nNote: You have to include an intercept manually using X = sm.add_constant(X).\n\n\n\n\n\n\n\n\n\n\nFigure 2.4: Example of logistic regression. (Probability of passing as a function of studying.)\n\n\n\n\nMathematical formula:\n\\[\\begin{align}\n  \\label{glmbernoulli}\n  y &\\sim \\mathsf{Bernoulli}(p)  \\\\ \\\\\n  \\tag{3b}\\log \\left( \\frac{p}{1-p} \\right) &= \\eta \\\\ \\\\\n  \\eta &= \\beta_0 + \\beta_1 \\cdot x\n\\end{align}\\]\nCode:\n\nRPython\n\n\n\nmodel &lt;- glm(y ~ x, data, family = \"binomial\")\n\n\n\n\nimport statsmodels.api as sm\nmodel = sm.GLM(y, X, family = sm.families.Binomial()).fit()\n\nNote: You have to include an intercept manually using X = sm.add_constant(X).\n\n\n\n\n\n\n\n\n\n\n\n\nSymbols & definitions\n\n\n\n\n\n\n\\(y\\) — response variable\n\nThe outcome.\n\nbinomial distribution\n\nSee  Binomial Distribution. \\(y \\sim \\mathsf{Binom}(n, \\, p)\\) means: The outcome follows a conditional4 binomial distribution with \\(n\\) trials and probability of success \\(p\\).\n\nBernoulli distribution\n\nIn case \\(n = 1\\), the binomial distribution simplifies to what is called the Bernoulli distribution.\n\nlink function\n\nA function that stretches the possible range of values of mean of the outcome to \\(-\\infty\\) to \\(+\\infty\\), which is convenient for estimating the model parameters, as it allows us to use a modified version of least squares (iteratively reweighted least squares).\n\ncanonical link function\n\nThe one link function that translates the linear predictor to the mean of the outcome. For the binomial distribution, this is the logarithm of the odds, or logit: \\(\\log \\left( \\frac{p}{1-p} \\right)\\)\n\n\\(\\eta\\) — linear predictor\n\nThe function that describes the relationship between the explanatory variable and the (transformed) mean of the outcome.\n\n\\(x\\) — explanatory variable\n\nThe variable the outcome is regressed on.\n\n\\(\\beta_0\\) — intercept\n\nValue of the outcome (\\(y\\)), when the explanatory variable (\\(x\\)) is zero. \\(\\hat{\\beta}_0\\) (beta-hat) is its estimate.\n\n\\(\\beta_1\\) — slope\n\nChange in the outcome (\\(y\\)), when the explanatory variable (\\(x\\)) increases by one. \\(\\hat{\\beta}_1\\) (beta-hat) is its estimate.\n\n\n\n\n\n\n\n\n\n\n\nExplanation\n\n\n\n\n\n(…)\nWhen collecting data, if you have the choice, always use the original number of successes and failures, not a proportion, or percentage! Computing a percentage throws away valuable information.\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nThis is example uses the ... data set. To use your own data, have a look here ).\nSuppose we want to estimate the relationship between the … and ….\n\nRPython\n\n\n…\n\n\n…"
  },
  {
    "objectID": "GLM.html#footnotes",
    "href": "GLM.html#footnotes",
    "title": "2  Generalized Linear Models",
    "section": "",
    "text": "Simple linear regression assumes the errors are normally distributed around the regression line. This is the same as saying that the outcome follows a normal distribution conditional on the model.↩︎\nSince \\(\\mu\\) depends on \\(x\\), the outcome follows a conditional normal distribution. Some write this as \\(y|x \\sim \\mathcal{N}\\!\\left(\\mu, \\, \\sigma^2 \\right)\\), but I opted for simpler notation here.↩︎\nSince \\(\\lambda\\) depends on \\(x\\), the outcome follows a conditional Poisson distribution. Some write this as \\(y|x \\sim \\mathsf{Pois}(\\lambda)\\), but I opted for simpler notation here.↩︎\nSince \\(p\\) depends on \\(x\\), the outcome follows a conditional binomial distribution. Some write this as \\(y|x \\sim \\mathsf{Binom}(n, \\, p)\\), but I opted for simpler notation here.↩︎"
  },
  {
    "objectID": "multiplelinearregression.html",
    "href": "multiplelinearregression.html",
    "title": "3  Multiple Linear Regression",
    "section": "",
    "text": "Linear regression with any number of explanatory variables.\n\nMultiple linear regression allows you to simultaneously take into account the effect of multiple explanatory variables.\n\n\nWhy not just several one-on-one analyses?\n\nA single model with all explanatory effects is usually preferable over fitting several simple linear regressions, because of omitted-variable bias, confounding, and Simpson’s paradox, among others:\n\nOmitted Variable BiasConfoundingSimpson’s Paradox\n\n\n\n\n\n\n\n\n\n\n\n\nSimple one-on-one analyses (also called correlation analysis) is prone to spurious findings. Multiple regression on the other hand, allows you to see what the effect of one variable is given the effect of all other included variables.\nThe simple fact is that almost everything correlates to some extent with everything else. Don’t believe me? Here is a correlogram of purely randomly drawn numbers:\n\nlibrary(\"corrplot\")\nset.seed(1)\nX &lt;- matrix(rnorm(100), nrow = 5, ncol = 20)\ncorrplot(cor(X))\n\n\n\n\nDark blue means a correlation close to \\(1\\) and dark red close to \\(-1\\). Note how many ‘strong’ correlations were found among independently drawn values."
  },
  {
    "objectID": "datamanagement.html",
    "href": "datamanagement.html",
    "title": " Data Management",
    "section": "",
    "text": "…"
  },
  {
    "objectID": "tidy.html",
    "href": "tidy.html",
    "title": "4  Tidy Format",
    "section": "",
    "text": "Collect data in a simple, consistent format called tidy data,[1] such that minimal effort is required to clean the data once you get to the analysis: Rows represent observations, columns represent the variables measured for those observations:\n\n\n\n\n\nThe basic principle of tidy data. This data set has 5 observations of 4 variables.\n\n\n\n\n\n\nGood example\n\nThe iris data set (preinstalled with R) is in tidy format:\n\n\n\nThe first 5 rows of the iris data set. Each row is a flower and each column a property.\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n...\n...\n...\n...\n...\n\n\n\n\n\n\n\nHere, the rows each represent one observation (a distinct flower), and the columns represent the variables measured/recorded (physical dimensions and species).\n\n\n\nBad example\n\nA common deviation from tidy format is to represent groups as columns:\n\n\n\nSystolic blood pressure (SBP) of 10 individuals. (untidy)\n\n\nwomen\nmen\n\n\n\n\n114\n123\n\n\n121\n117\n\n\n125\n117\n\n\n108\n117\n\n\n122\n116\n\n\n\n\n\n\n\nDo you have different groups? Time points? Replicates of the experiment? Then try to adhere to the same principle: Columns are variables. Simply add a variable that indicates which group/time point/replicate this observation belongs to:\n\n\n\nSex and systolic blood pressure (SBP) of 10 individuals. (tidy)\n\n\nsex\nSBP\n\n\n\n\nfemale\n114\n\n\nfemale\n121\n\n\nfemale\n125\n\n\nfemale\n108\n\n\nfemale\n122\n\n\nmale\n123\n\n\nmale\n117\n\n\nmale\n117\n\n\nmale\n117\n\n\nmale\n116\n\n\n\n\n\n\n\n\n\n\nConverting to tidy format\n\nIf you have data split by group/time point/replicate here is how you can convert it to tidy format:\n\n# The untidy data set\nUntidy\n\n  women men\n1   114 123\n2   121 117\n3   125 117\n4   108 117\n5   122 116\n\n# Convert by hand\nTidy &lt;- data.frame(\n  sex = rep(c(\"female\", \"male\"), each = nrow(Untidy)),\n  SBP = c(Untidy$women, Untidy$men)\n)\n\n# Convert using a package (install if missing)\nlibrary(\"reshape2\")\nmelt(Untidy)\n\n   variable value\n1     women   114\n2     women   121\n3     women   125\n4     women   108\n5     women   122\n6       men   123\n7       men   117\n8       men   117\n9       men   117\n10      men   116\n\n\nIf you have a simple data set like the one shown here, converting with the package reshape2 is easiest. Converting by hand may be slightly more work, but I prefer it because you can easily see what’s going on, add more variables if needed, etc.\n\n\n\n\n\n[1] H. Wickham, “Tidy data,” Journal of Statistical Software, vol. 59, no. 10, 2014, doi: 10.18637/jss.v059.i10."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "[1] H.\nWickham, “Tidy data,” Journal of Statistical\nSoftware, vol. 59, no. 10, 2014, doi: 10.18637/jss.v059.i10.\n\n\n[2] J.\nB. Finlay, X. Liu, R. W. Ermel, and T. W. Adamson, “Maternal Weight\nGain as a Predictor of Litter\nSize in Swiss Webster,\nC57BL/6J, and\nBALB/cJ\nmice,” J Am Assoc Lab Anim Sci, vol. 54, no. 6,\npp. 694–699, Nov. 2015."
  },
  {
    "objectID": "univariatetests.html",
    "href": "univariatetests.html",
    "title": " Univariate Tests",
    "section": "",
    "text": "Modeling of real-world problems almost always involves multiple variables, which is why this book focuses on regression analysis. Yet even in large regression models, the eventual comparisons made still often boil down to simple univariate tests. This chapter serves as reference material for those tests.\n\n\nWhich tests should I know?\n\nThere is little benefit in memorizing a large number of tests. If you know how to put to words what you want to compare, you can just look up how to do it. However, some univariate tests keep reappearing, even later when you start learning about more complex models. It is therefore important to understand the following basic tools and their typical use-cases:\n\n\\(t\\)-test\n\nCompare means\nCompare regression coefficients to zero\n\n\\(\\chi^2\\)-test (the Greek letter \\(\\chi\\), pronounced ‘kai’)\n\nCompare observed and expected frequencies\nPerform a goodness-of-fit test\n\n\\(F\\)-test\n\nCompare variances\nPerform an omnibus test"
  },
  {
    "objectID": "t-test.html#means",
    "href": "t-test.html#means",
    "title": " T-test",
    "section": "Means",
    "text": "Means\n\n\n\n\n\nFigure 1: The location parameter of the normal distribution is the mean \\(\\mu\\). It can be compared with a \\(t\\)-test.\n\n\n\n\nMathematical formula:\n\\[\\begin{align}\n  \\tag{1}\\label{ttest}\n  t = \\frac{\\bar{y}_1 - \\bar{y}_2}{\\mathrm{SE}}\n\\end{align}\\]\nCode:\n\nRPython\n\n\n\n# Formula depends on type, see examples.\nt.test(y ~ x, data)\n\n\n\n\n# Formula depends on type, see examples.\nfrom scipy.stats import ttest_ind\nttest_ind(a, b, equal_var = False)\n\nNote: a and b have to be the values in either groups, not the outcome and grouping variable.\n\n\n\n\n\nSymbols\n\n\n\\(t\\): The \\(t\\)-value (test statistic from which a \\(p\\)-value is computed)\n\\(\\bar{y}_1\\): Mean of group 1\n\\(\\bar{y}_2\\): Mean of group 2\n\\(\\mathrm{SE}\\): Standard error of the difference (calculation depends on the type of \\(t\\)-test)\n\n\n\n\nExplanation\n\nAlso see the video on \\(t\\)-tests .\nSuppose we draw 15 random males and females:\n\n\n\n\n\nAll we have is a sample. You can calculate the sample mean (\\(\\bar{y}\\)) of a group, but that is only an estimate of the true group mean (\\(\\mu\\)). Since we have estimates, there is uncertainty.\nOur level of evidence \\(t\\) is then the difference in estimated group means, divided by the uncertainty of the estimated difference:\n\\[t = \\frac{\\text{difference in means}}{\\text{uncertainty}}\\]\nHow this uncertainty (called the standard error) is calculated, depends on the type of \\(t\\)-test used.\nOnce you have a \\(t\\)-value, you can use it to compute a \\(p\\)-value:"
  },
  {
    "objectID": "chisq-test.html",
    "href": "chisq-test.html",
    "title": " Chi-squared test",
    "section": "",
    "text": "…"
  },
  {
    "objectID": "f-test.html",
    "href": "f-test.html",
    "title": " F-test",
    "section": "",
    "text": "…"
  }
]